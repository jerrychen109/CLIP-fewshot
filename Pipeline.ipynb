{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "metadata": {
      "interpreter": {
        "hash": "2a41893882cd6a4f9de9c4407ee80149143837d532b553c8457b9b80809c1765"
      }
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerrychen109/cs197/blob/master/Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y10p9U6OhstD"
      },
      "source": [
        "MOUNT DRIVE + CONNECT GITHUB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs5lnQcwW8q7",
        "outputId": "e2df2c97-aecb-4859-c24c-1913fd1a8b08"
      },
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwf8s_YmVVRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d40520-57e3-4a0f-de19-55dd0dab4652"
      },
      "source": [
        "# FOLDERNAME = \"CS197\"\n",
        "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "import sys\n",
        "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
        "# ! git clone \"https://USERNAME:PASSWORD@github.com/jerrychen109/cs197.git\"\n",
        "# # NEED TO FIND OUT BETTER WAY (WITH TOKENS??) ^^\n",
        "# FOLDERNAME = \"3_SPR/cs197/fewshot-code\"\n",
        "FOLDERNAME = \"CS197/cs197/\"\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "# Commands for local changes error\n",
        "# ! git config --global user.email \"githubEMAIL\"\n",
        "# ! git config --global user.name \"githubUSERNAME\"\n",
        "# ! git stash push\n",
        "# ! git stash drop"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CS197/cs197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBiyaIBTUUXR"
      },
      "source": [
        "# ! git pull \"https://USERNAME:PASSWORD@github.com/jerrychen109/cs197.git\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTSSeXZEOFGu"
      },
      "source": [
        "# %cd datasets\n",
        "# !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "# !tar -xzf cifar-10-python.tar.gz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMO2rSSNOUyL"
      },
      "source": [
        "# ! git config --global user.email \"\"\n",
        "# ! git config --global user.name \"\"\n",
        "# ! git commit . -m \"\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5aBrRkaPysw"
      },
      "source": [
        "# ! git push"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKRHS0kxiPRU"
      },
      "source": [
        "IMPORT LIBRARIES AND MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma1oVYMNeAVC",
        "cellView": "form",
        "outputId": "144d3cb6-93ee-4103-8dce-95cfe7cdf5f7"
      },
      "source": [
        "#@title\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "from collections import OrderedDict\n",
        "import IPython.display\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "import skimage #Has some images in here - check original \"Interacting with CLIP.ipynb\" document\n",
        "import torch\n",
        "\n",
        "from prototype import Prototype\n",
        "from prototypevector import PrototypeVector\n",
        "from torchvision.datasets import CIFAR10, CIFAR100\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from utils.data_utils import *\n",
        "from utils.image_utils import *\n",
        "from utils.text_utils import *\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "--2021-05-11 20:06:45--  https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.253.38, 13.107.226.38, 2620:1ec:29::38, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.253.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356917 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘bpe_simple_vocab_16e6.txt.gz’\n",
            "\n",
            "bpe_simple_vocab_16 100%[===================>]   1.29M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-05-11 20:06:45 (16.7 MB/s) - ‘bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n",
            "\n",
            "CUDA version: 11.0\n",
            "Torch version: 1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrLC2hunVVRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "f6af870b-bafd-46e4-d663-1f6e15e95a7a"
      },
      "source": [
        "#@title\n",
        "CIFAR10_DIR = 'datasets/cifar-10-batches-py'\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(os.path.join(FOLDERNAME, CIFAR10_DIR)))\n",
        "TRAIN_BATCHES = [os.path.join(CIFAR10_DIR, batch_path) for batch_path in [\n",
        "    'data_batch_1',\n",
        "    'data_batch_2',\n",
        "    'data_batch_3',\n",
        "    'data_batch_4',\n",
        "    'data_batch_5'\n",
        "]]\n",
        "TEST_BATCH = os.path.join(CIFAR10_DIR, 'test_batch')\n",
        "\n",
        "train_data, train_labels = load_cifar10(TRAIN_BATCHES)\n",
        "test_data, test_labels = load_cifar10_batch(TEST_BATCH)\n",
        "print(\"train shape: \", train_data.shape)#[0])\n",
        "print(\"test shape: \", test_data.shape)#[0])\n",
        "\n",
        "train_data_dict = sample_classes(train_data, train_labels, per_class = 250)\n",
        "for c in train_data_dict:\n",
        "  train_data_dict[c] = resize_images(train_data_dict[c])\n",
        "MODELS = {\n",
        "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
        "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
        "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
        "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",    \n",
        "}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape:  torch.Size([50000, 3, 32, 32])\n",
            "test shape:  torch.Size([10000, 3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K53T_iHZd7-W",
        "cellView": "form",
        "outputId": "d2e04ed7-274c-49cb-b29b-592b837359b3"
      },
      "source": [
        "#@title\n",
        "! wget {MODELS[\"ViT-B/32\"]} -O model.pt\n",
        "model = torch.jit.load(\"model.pt\").cuda().eval()\n",
        "input_resolution = model.input_resolution.item()\n",
        "context_length = model.context_length.item()\n",
        "vocab_size = model.vocab_size.item()\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-11 20:06:54--  https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.253.38, 13.107.226.38, 2620:1ec:29::38, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.253.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353976522 (338M) [application/octet-stream]\n",
            "Saving to: ‘model.pt’\n",
            "\n",
            "model.pt            100%[===================>] 337.58M  21.2MB/s    in 13s     \n",
            "\n",
            "2021-05-11 20:07:08 (25.2 MB/s) - ‘model.pt’ saved [353976522/353976522]\n",
            "\n",
            "Model parameters: 151,277,313\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg8ArUFPig44"
      },
      "source": [
        "Calculate image mean and standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh3rtIhrd-el",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cbd06d-2cad-4d2c-971a-6c6456ec44d2"
      },
      "source": [
        "image_mean = getImageMean(train_data).cuda()\n",
        "image_std = getImageStd(train_data).cuda()\n",
        "print (\"image mean: \", image_mean)\n",
        "print (\"image_std: \", image_std)\n",
        "# image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "# image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
        "##### IMPORTANT!!!!! NEED TO CHANGE THIS DEPENDING ON DATASET!!!!! #######"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image mean:  tensor([0.4914, 0.4822, 0.4465], device='cuda:0')\n",
            "image_std:  tensor([0.2470, 0.2435, 0.2616], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM1w9OVGiqgY"
      },
      "source": [
        "Initialize tokenizer\n",
        "\n",
        "Also: create descriptions, find filenames and graph images with labels and descriptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1kch4FmlEY7",
        "collapsed": true
      },
      "source": [
        "tokenizer = SimpleTokenizer()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Y6CYVProj7QQ"
      },
      "source": [
        "#@title\n",
        "# descriptions = {\n",
        "#     \"page\": \"a page of text about segmentation\",\n",
        "#     \"chelsea\": \"a facial photo of a tabby cat\",\n",
        "#     \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "#     \"rocket\": \"a rocket standing on a launchpad\",\n",
        "#     \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
        "#     \"camera\": \"a person looking at a camera on a tripod\",\n",
        "#     \"horse\": \"a black-and-white silhouette of a horse\", \n",
        "#     \"coffee\": \"a cup of coffee on a saucer\"\n",
        "# }\n",
        "# filenames = getImageFilesFromDir(skimage.data_dir)\n",
        "# filenames\n",
        "# filenamesInDescriptions = sorted([x for x in filenames if x[:-4] in descriptions])\n",
        "# images = getImagesFromFiles(skimage.data_dir, filenamesInDescriptions)\n",
        "# labels = sorted(list(descriptions.keys()))\n",
        "# _ = graphImages(images, texts=labels, descriptions=descriptions)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3VTqKJGj_JO"
      },
      "source": [
        "Declare and initialize PrototypeVector\n",
        "- Add image mean and std so images can be standardized when passed into PrototypeVector\n",
        "- Add dict of training data (should we modify this to take in less?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPDB6l4EVVRQ"
      },
      "source": [
        "vector = PrototypeVector(model.encode_image, device, image_mean, image_std, k=1)\n",
        "vector.addPrototypesFromDict(train_data_dict)\n",
        "# vector.addPrototypesWithFilenames([skimage.data_dir]*len(labels), filenames_for_class, labels)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ICSkqltv8gn"
      },
      "source": [
        "- Take smaller sample of test data\n",
        "- Resized images (can we do multiple images at once??)\n",
        "- Create image encodings (good idea to do it here to save re-computation in prototypevector later)\n",
        "TODO: Jerry - standardize test data (although my changes to imagesToVector may have resolved this\n",
        "- Take a look at Github repo - I switch from tensors to np.array to back; we should figure out what to do if we want to increase our test_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laqJKFzJydDq"
      },
      "source": [
        "del train_data #Save room in GPU?"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm_ndsBNn4Jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bd822a-1d0c-4a3f-fd29-42f2477ab6b4"
      },
      "source": [
        "# classification accuracy against test set\n",
        "print(test_data.shape)\n",
        "test_data_small = test_data[:1000]\n",
        "test_labels_small = test_labels[:1000]\n",
        "\n",
        "_, encoded_images = imagesToVector(test_data[:1000], \n",
        "                                   model.encode_image, \n",
        "                                   device=device, \n",
        "                                   image_mean=image_mean, \n",
        "                                   image_std=image_std)\n",
        "# Original (using old classify)\n",
        "# preds = []\n",
        "# nearests = []\n",
        "# encoded_images = []\n",
        "# for image, c in tqdm(zip(test_data, test_labels), total=len(test_data)):\n",
        "#   image = resize_images(image.unsqueeze(0))\n",
        "#   test_encoded = encodeImageWithFunc(model.encode_image, image).squeeze()\n",
        "#   encoded_images.append(test_encoded)\n",
        "#   label, nearest = vector.classify(cosineSimilarity, test_encoded)\n",
        "#   preds.append(label)\n",
        "#   nearests.append(nearest)\n",
        "\n",
        "# Standardizing attempt (uses a lot of memory, dimensions not lining up I'm not sure how to get correct dimensions using unsqueeze(0))\n",
        "# Ideally data_images is a list of images but there are some dimension issues\n",
        "\n",
        "# data_images = []\n",
        "# for image, c in tqdm(zip(test_data_small, test_labels_small), total=len(test_data_small)):\n",
        "#   image = resize_images(image.unsqueeze(0))\n",
        "#   data_images.append(image) \n",
        "# data_images = torch.tensor(np.stack(images)).cuda()\n",
        "# data_images -= image_mean[:, None, None]\n",
        "# data_images /= image_std[:, None, None]\n",
        "# data_images = encodeImageWithFunc(model.encode_image, data_images)\n",
        "# data_images = normalize(data_images)\n",
        "\n",
        "#Second Attempt\n",
        "# encoded_images = []\n",
        "# for image, c in tqdm(zip(test_data, test_labels), total=len(test_data)):\n",
        "#   image = resize_images(image.unsqueeze(0))\n",
        "#   test_encoded = encodeImageWithFunc(model.encode_image, image).squeeze()\n",
        "#   encoded_images.append(test_encoded)\n",
        "print(encoded_images.shape)\n",
        "\n",
        "# Pehaps we can calculate only part of encoed_images at a time\n",
        "# (1000 at a time in order to generate all 10000 without putting too much RAM on the GPU)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCUsAnfqcQvi"
      },
      "source": [
        "tuples = vector.classifyImagesWithClassVector(cosineSimilarity, encoded_images, k=100, recalc=False)\n",
        "preds = np.array([t[0] for t in tuples])\n",
        "test_acc = np.mean(preds == np.array(test_labels)[:1000])\n",
        "print(\"test accuracy: \", test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfW4xRRiE4SY"
      },
      "source": [
        "tuples = vector.classifyImagesWithClassVector(cosineSimilarity, encoded_images, k=10, recalc=False)\n",
        "preds = np.array([t[0] for t in tuples])\n",
        "test_acc = np.mean(preds == np.array(test_labels[:1000]))\n",
        "print(\"test accuracy: \", test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OxlrnPffqfN"
      },
      "source": [
        "def classifyImages(images, trueLabels, k, iter, recalc=True):\n",
        "  iterAccuracies = np.zeros(iter)\n",
        "  numLabelCorrect = np.zeros(len(trueLabels))\n",
        "  for i in range(iter):\n",
        "    tuples = vector.classifyImagesWithClassVector(cosineSimilarity, images, k=k, recalc=recalc)\n",
        "    pred_labels = np.array([tup[0] for tup in tuples])\n",
        "    matching = (pred_labels == np.array(trueLabels))\n",
        "    iterAccuracies[i] = np.mean(matching)\n",
        "    accurate_labels = pred_labels[matching]\n",
        "    # NARVIN: Use Counter here to get accuracies per predicted label and per actual label!\n",
        "  totalAccuracy = np.mean(iterAccuracies)\n",
        "\t# for i in range(len(numLabelCorrect)):\n",
        "\t\t# labelAccuracies.append((numLabelCorrect[i] / iter), label[i])\n",
        "  return totalAccuracy, iterAccuracies#, labelAccuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug3n1K37i1iZ"
      },
      "source": [
        "classifyImages(encoded_images[:100], test_labels[:100], 1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HejaI8ypSwcz"
      },
      "source": [
        "multRuns = {}\n",
        "for k in tqdm(range(1, 101)):\n",
        "  multRuns[k] = classifyImages(encoded_images[:1000], test_labels[:1000], k, 10, recalc=True)\n",
        "# multRuns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckCShZEBqfcV"
      },
      "source": [
        "data = np.array([(key, np.array(value[1])) for key, value in multRuns.items()], dtype=object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1w9v5BjojgJ"
      },
      "source": [
        "df = pd.DataFrame.from_records(np.array(data), columns=['k', 'Accuracy'])\n",
        "df = df.explode(\"Accuracy\")\n",
        "# df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Ee4uBTlqvj"
      },
      "source": [
        "ax = sns.barplot(x=\"k\", y=\"Accuracy\", data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSQhU4OeBAZr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}